\section{Introduction}
The \textit{Approximate Similarity Search Problem} regards efficiently finding a set $A\in \mathcal{F}$ from some corpus $\mathcal{F}$ consisting of $n$ subsets of some universe $U$ that is approximately similar to a query set $Q$ in regards to the \textit{Jaccard Similarity} metric $J(A,Q) = \frac{|A\cap Q|}{|A\cup Q|}$\cite{dahlgaard2017fast}\cite{fast-similarity-search}. Approximate means that given two variables $0 < j_2 < j_1 < 1$, the algorithm should efficiently find some set $A\in \mathcal{F}$ such that $J(A, Q) \geq j_2$ if there exists some set $B\in \mathcal{F}$ where $J(B,Q)\geq j_1$.
Practical applications includes searching through large corpora of high-dimensional text documents like plagiarism-detection or website duplication checking among others\cite{vassilvitskii2018}. The main bottleneck in this problem is the \textit{curse of dimensionality}. 
A trivial algorithm can solve this problem in $O(n|Q|\max_{A\in \mathcal{F}}{|A|})$ time (where $n$ is the amount of elements in the corpus $\mathcal{F}$), but algorithms with a query-time proportional to the dimensionality of the corpus $\max_{A\in \mathcal{F}}|A|$ scale poorly when working with high-dimensional data sets. 
Text documents are especially bad in this regard since they often are encoded using \textit{$w$-shingles} ($w$ contiguous words) which \citet{li2011hashing} shows easily can reach a dimensionality upwards of $|Q|=2^{83}$ using just $5$-shingles.\\
The classic solution to this problem is the \textit{MinHash} algorithm presented by \citet{broder1997minhash} to perform website duplication checking for the AltaVista search engine. 
It pre-processes the data once using hashing to perform effective querying in $O(n^\rho\log(n)|Q|)$ time (where $\rho = \frac{\log_2{(1/j_1)}}{\log_2{(1/j_2)}}$), a significant improvement independent of the dimensionality of the corpus.
Many improvements have since been presented to both improve pre-processing time, query time and space efficiency. 
Notable mentions includes (but are not limited to) the use of \textit{tensoring}\cite{andoni2006efficient}, \textit{b-bit minwise hashing}\cite{ping2011theory}, \textit{fast similarity sketching}\cite{dahlgaard2017fast}. 
Applications of these techniques lead to efficient querying in $O(n^\rho + |Q|)$ time, with a constant error probability. If one wishes to achieve an even better error probability such as $\varepsilon = o(1)$, it is standard practice within the field to use $O(\log_2(1/\varepsilon))$ independent data structures and return the best result, resulting in a query time of $O(\log_2(1/\varepsilon) (n^\rho + |Q|))$. 

Recent advances by \citet{fast-similarity-search} show that it is possible to achieve an even better query time by sampling these data structures from one large sketch, leading to a query time of $O(n^\rho \log_{1/\varepsilon} + |Q|)$.

Each of the data structures will return a list of candidate sets, of which false positives needs to be eliminated from. An efficient filtering scheme can choose exactly 1 set from each data structure as a candidate. Then, the final result can be picked by eliminating data structures that are highly probable to return a bad candidate. This final step requires computing the cardinality of a list of bit-strings (i.e. amount of bits set per bit-string) very efficiently, for which an algorithm is presented by \citet{fast-similarity-search} that brings the query time down to $O((\frac{n\log_2 w}{w})^\rho \log(1/\varepsilon) + |Q|)$ where $w$ is the word size.\\
The main focus of this project is to analyze, prove, implement and evaluate this parallel bit-counting technique. The analysis will be based on the original paper \cite{fast-similarity-search}, but with some modifications to resolve some of the issues with the original algorithm. This will also include a pseudo-code implementation of the algorithm since the original paper only describes it through recurrences. This leads to a proof of correctness that slightly differs from the one presented in the paper, and a time complexity analysis that does indeed show the sub-linear running time as claimed.\\
The theoretical analysis will be backed up by a real-life implementation that can be benchmarked to help show this sub-linear running time in practice. At last, reflections on the results and methods will be made to back up eventual conclusions.

