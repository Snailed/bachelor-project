\section{Parallel Bit Counting}
In the same article where \citet{fast-similarity-search} presents the improvements for algorithms with an $o(\varepsilon)$ error probability, a parallel algorithm to compute the cardinality of a bit-string is also presented. It is only described by recurrences, and omits any implementation details. I will try to describe the same algorithm in a much simpler fashion and prove its correctness and time complexity while doing it. This means that this section is my own interpretation of the algorithm, proofs and analysis described in \cite{fast-similarity-search}.
A naive algorithm to perform bit-counting could be described like in algorithm \ref{alg:naive-cardinality}.
\begin{algorithm}[H]
\caption{A naive linear time algorithm}\label{alg:naive-cardinality}
\begin{algorithmic}[1]
\Function{Cardinality}{$w$, $d$} \Comment{$w$ is the input word, $d$ is the word-size}
\State $x \gets 0$
\State $i \gets 0$
\While{$i \leq d$}
\State $x \gets x + (w \gg i) \land 1$
\State $i \gets i + 1$
\EndWhile
\State \Return $x$
\EndFunction
\end{algorithmic}
\end{algorithm}
This algorithm trivially runs in linear time to the dimensionality\footnote{For the rest of this project, it is assumed that $d$ is a power of 2.} $d$ of the input word $w$. For $n$ words of size $d$, this algorithm runs in $O(nd)$ time.
\citet{fast-similarity-search} presents two improvements to this: The first will improve the running time to $O(n\log(d))$ by utilizing a divide-and-conquer technique and the second to $O(n + \log(d))$ time by introducing parallelism.
\subsection{Divide-and-Conquer}
To explain how divide-and-conquer methods can be used to improve running time, we must first introduce a bit mask from \cite{fast-similarity-search} defined like so:
$$m_{i,j} = \underbrace{0\dots 0}_{2^{j}-2^{i}}\underbrace{1\dots 1}_{2^i}\dots\underbrace{0\dots 0}_{2^{j}-2^{i}}\underbrace{1\dots 1}_{2^i}$$
where $j > i$ and $j$ and $i$ are non-negative integers. The notation $m_{i}$ is a shorthand for $m_{i, i+1}$ and indicates a mask with an equal amount of 1's and 0's. By computing $w \land m_{i,j}$ for some word $w$ and some integers $i, j$, we can replace every other sub-string of size $2^i$ in the bit-string with 0's. This will be described as \textit{isolating} segments of size $2^i$.\\
We will use this in the following operation:
\begin{equation}
    T(w, m, k) = (w\gg k) \land m
\end{equation}
This operation isolates the segments indicated by the bit-mask starting from the $k$th position of the word $w$. This is useful for partitioning a bit-string, since any word now can be described as
$$w=T(w, m_i, 0) + (T(w, m_i, 2^i) \ll 2^i)$$
The algorithm to calculate the cardinality can then be described like in algorithm \ref{alg:divide-and-conquer-cardinality}
\begin{algorithm}[H]
\caption{A divide-and-conquer approach}\label{alg:divide-and-conquer-cardinality}
\begin{algorithmic}[1]
\Function{Cardinality}{$w$, $d$} \Comment{$w$ is the input word, $d$ is the word-size}
\State $i \gets 0$
\While{$i \leq \log_2(d)$}
\State $w \gets T(w, m_i, 0) + T(w, m_i, 2^i)$
\State $i \gets i + 1$
\EndWhile
\State \Return $w$
\EndFunction
\end{algorithmic}
\end{algorithm}
The simplest way to gain intuition of this algorithm is to prove it. A loop-invariant is presented like so:
\begin{invariant}
\label{thm:divide-invariant}
At the $i$th iteration of the algorithm, each bit-string-segment of size $2^i$ of the word $w^{(i)}$ will contain the cardinality of the corresponding segment of the word $w^{(0)}$. Furthermore, the operation can be done in constant time.
\end{invariant}
\begin{proof}
    When $i=0$, then each bit-string of size $2^0=1$ in $w^{(0)} = w$ will be $1$ if the bit is $1$ and $0$ if the bit is $0$. This proves our base case. \\
    If at step $i$ the word $w^{(i)}$ contains $\frac{d}{2^i}$ segments of size $2^i$, then invariant \ref{thm:divide-invariant} will be upheld if we combine each segment with its neighbor to form a segment of size $2^{i+1}$. Since the cardinality of a combined bit-string is equal to the cardinality of its parts, we can divide all of the segments into pairs of size $2^{i+1}$ and add them together to form the new pair, which is done by the operation $T(w, m_i, 0) + T(w, m_i, 2^i)$ in constant time if the masks are pre-computed. This operation works because $T(w, m_i, 0)$ isolates every other segment of size $2^i$ starting from the first segment and $T(w, m_i, 2^i)$ isolates every other segment of size $2^i$ starting from the second segment.\\
\end{proof}
When the algorithm terminates after $\log_2(d)$ iterations, the segments will have size $2^{\log_2{(d)}} = d$ and thus span the entire original word, which means that we have the bit-count of the original word. % TODO: Skriv noget omkring hvad der sker n√•r d ikke er en faktor af 2
% \input{tikz/example1}
\subsection{Parallelism}
To introduce parallelism into the algorithm, we must first realize the following: When two segments of $2^i$ bits gets combined, they will not need all of the $2^{i+1}$ bits to represent their sum. It is actually such that the bits used at iteration $i$ is exactly $i+1$.
\begin{invariant}
    \label{thm:amount-of-bits}
    At the $i$th iteration of the algorithm, the amount of bits set in a given segment of a word is at most $i+1$.
\end{invariant}
\begin{proof}
    We will prove this by induction. \\
    At $i=0$, the size of the segments are $2^0 = 1$, which is equal to $i+1$.\\
    If it is true at iteration $i$, then at iteration $i+1$ the algorithm will add two words of size $i+1$ which creates a word of size $i+2$, which fulfills the loop invariant.
\end{proof}
Now, we will introduce a function $l(i)$, which produces the smallest number such that $2^{l(i)} \geq i + 2$. If we use the bit mask $m_{l(i), i+1}$ instead of $m_{i}$, we will get the same result. This also means that we can pack $2^{i-l(i)}$ words into one by utilizing the empty space in each segment.
\begin{algorithm}[H]
\caption{A parallel divide-and-conquer algorithm}\label{alg:parallel-d-and-c}
    \begin{algorithmic}[1]
        \Function{Compute}{$S$, $d$} \Comment{$S$ is the input set, $d$ is the word-size}
            \State $n \gets S.length$
            \For{$i \in [\log_2(d)]$}
                \State $k' \gets \{\}$
                \State $t \gets \{\}$
                \For{$w \in S$}
                    \State $k' \gets k' \cup \{T(w, m_i, 0) + T(w, m_i, 2^i)\}$
                \EndFor
                \If{$i < 2$}
                    \State $S \gets k'$
                \Else
                \If{$l(i) = l(i+1)$}
                    \For{$j \in [\lceil S.length / 2 \rceil]$}
                    \State $t \gets t \cup \{k'_{2j} + (k'_{2j+1} \ll 2^i)\}$ % TODO: Er det l(i) eller l(i+1)?
                    \EndFor
                \Else
                    \For{$j \in k'$}
                        \State $t \gets t \cup \{T(k'_{j}, m_{l(i)}, 0) + (T(k'_{j}, m_{l(i)}, 2^{l(i)}) \ll 2^{i})\}$
                    \EndFor
                \EndIf
                \State $S \gets t$
                \EndIf
            \EndFor
            \State $S' \gets \{\}$
            \For{$j$ in $[S.length]$} \Comment{For every word in the final set}
            \For{$k$ in $[2^{l(\log_2(d))}]$} \Comment{For every original word embedded in $S_j$}% TODO: Research this!
                \State $S' \gets S' \cup \{T(S_j, m_{l(\log_2(d)), \log_2(d) + 1}, k\cdot 2^{l(\log_2(d))})\}$
                \EndFor
            \EndFor
            \State \Return $S'$
        \EndFunction
    \end{algorithmic}
\end{algorithm}
This algorithm is quite a bit more complex than the first one, and to prove it we need some more terminology.\\
When we pack the words $t_0, \dots, t_{2^{i-l(i)}-1}$ into a word $t$, we say that $t$ is $i$-packed. This means one can extract all $2^{i-l(i)}$ different words by performing the operation:
\begin{equation}
    \label{eq:extract-from-embed}
t_j=T(t, m_{l(i), i+1}, j\cdot 2^{l(i)})
\end{equation}
for all $j\in [2^{i-l(i)}-1]$.
The point of the algorithm is to ensure that after every step of the algorithm that every word in $S$ is $(i+1)$ packed. When the first loop of the algorithm terminates at $i=\log_2(d)-1$, then every word will be $\log_2(d)$-packed, which means that it fits $2^{\log_2(d)-l(\log_2(d))}$ words. When working with 64-bit words, this will result in $2^{6-3}=8$ words per packed word.
\begin{invariant}
    At the end of the $i$th iteration of the first loop of the algorithm, every word in $S$ will be $(i+1)$-packed for any $i \geq 1$
\end{invariant}
\begin{proof}
    First, the first two iterations of the loop are run, such that $i=1$. During the first two iterations of the loop, no words are combined since it just uses the naive algorithm, which upholds the invariant because any $2$-packed words embeds $2^{0}=1$ of the original words.\\
    For any iteration where $i\geq 2$, one out of two things can happen. Either, $l(i+1) = l(i)$ (we do not need more bits to describe each segment), or $l(i + 1) = l(i) + 1$ (we need twice the amount of bits to describe each segment).\\
    In the first case, we will combine the words using bit-shifting. Since every word in $k'$ uses the same amount of bits to represent each segment even though we just combined segments when creating $k'$, there must be an empty space of at least size $l(i+1)$ in every segment. We can fill out that empty space by adding another word bit-shifted by a factor of 2. % TODO: Find ud af om det er 2^i eller 2^l(i)
    Now, the amount of words embedded is equal to the sum of both of its parts (i.e. has doubled). This upholds the loop invariant since any $(i+1)$-packed word must have twice the amount of words embedded than a $i$-packed word if $l(i+1) = l(i)$ by definition.\\
    In the second case, $l(i + 1) = l(i) + 1$. In this case, we need to relocate our segments that actually use $2^{l(i)}$ bits so they fit into the segments of size $2^i$ bits that were created when creating $k'$.
    The same amount of words have been embedded into each words as before, but as $l(i+1) = l(i) + 1$ a $(i+1)$-packed word should only embed the same amount of words as a $i$-packed word, which upholds the invariant.
    Furthermore, since we relocated the segments, we can now still extract each original word by performing the operation in equation \ref{eq:extract-from-embed}.\\
    The algorithm will terminate when $i = \log_2(d) - 1$, from which each word will embed $2^{\log_2(d)-l(\log_2(d))}$ of the original words.
\end{proof}
\subsection{Discrepansies}
In the original article, combining the sections of a word (like we do in algorithm \ref{alg:divide-and-conquer-cardinality} and on line 7 and 14 in algorithm \ref{alg:parallel-d-and-c}) is done with the following operation:
$$t^{(i+1)}=T(T(t^{(i)}, m_i, 0) + T(t^{(i)}, m_i, 2^i), m_{i+1}, 0)$$
Here, the algorithm is described as a recurrence, where $t^{(i)}$ is the word $t$ after $i$ iterations. This definition is troublesome since the outermost call to $T$ causes information loss. A counter-example to this is just the word $t^{(0)} = 0\texttt{b}1111$. We can see that this word contains 4 bits set. Using this operation, we can see that 
$$t^{(1)}=T(T(0\texttt{b}1111, 0\texttt{b}0101, 0) + T(0\texttt{b}1111, 0\texttt{b}0101, 2), 0\texttt{b}0011, 0) = 0\texttt{b}0010$$
The outermost application of the bit mask means that we lose information about the two most significant bits of $t$. If we were to calculate $t^{(2)}$ and thus let the algorithm terminate, we would only have counted half of all the bits!\\
This is simply solved by removing the outermost call to $T$, since no information will be lost and invariant \ref{thm:divide-invariant} will apply.\\
Another issue with the original article is that the order of the cardinalities in the final set $S'$ is not consistent with the order of the original set $S^{(0)}$.
When the algorithm shifts the sections in each word in line 17-19 of the algorithm, every other section gets shifted $2^i$ bits to the left. 
When the algorithm has terminated, the cardinality of $S^{(0)}_i$ might be contained in $S'_j$ where $i\not = j$. The original paper by \citet{fast-similarity-search} does not mention this, and it is not said whether this affects the criteria of correctness, but for any practical usage, this does seem like it could be an issue. 
This is only a problem when $\log_2(d) \leq 7$, since the affected branch is only run when $l(i + 1) = l(i) + 1$ and $i > 2$ (it is also run when $i=2$, but causes no problem since no words have been packed yet, which means that they will not get shifted). In the case where $d=128$, the following formula calculates an index $j$ from $i$ such that $|S_i|=S'_j$.\\
$$
j(i) =
\begin{cases}
    (i \gg 1) + ((i \gg 3) \ll 2) & \textrm{if }i \& 1 = 0 \\
    (i \gg 1) + 4 + ((i \gg 3) \ll 2) & \textrm{otherwise}
\end{cases}
$$
\begin{proof}
When $i=6$, every word in $S$ contains $2^{6-3}=8$ words. Before lines 17-19 are run, each word from the original set is described as one bit-string segment of $2^{l(6)}=8$ bits each.
Lines 17-19 then takes every other segment and shifts it $2^6=64$ bits to the left. Since this is the last iteration of the algorithm, the for loop in lines 25-29 gets run immediately after, which extracts each segment into $S'$ in the order of the least significant first for each word.\\
This means that every other word from the original set will be shifted 4 indices later in $S'$, since 64 bits can contain $64/8=8$ words and half of them get shifted as well. 
%When one calculates $j(i)$, and i is less than 8 and i is even, then we know that it has not been shifted. This means that the index we need to calculate must be one of the first 4 indices of $S'$. Since the order of the even elements of the original set is preserved, then we can just calculate half of the index to find 
The calculation presented to find $j(i)$ has two branches: one branch if $i$ is even and one if $i$ is uneven. If $i$ is even, $j(i)$ can be found by calculating $(i \gg 1) + ((i\gg 3) \ll 2)$ (as is done using bit-shift calculation). If $i$ is odd, then this calculation is just offset by 4.
This works, because the shifting makes such that $S'$ contains series of the first 4 even indices of the original set, then the first 4 odd indices, then the next 4 even indices and so on. The $(i \gg 3)$ calculates the index of which of these series that $i$ is found in, and by shifting it 2 bits to the left, we effectively multiply this index by 4 to find which series that our number is in. Then we calculate which of these 4 indices in the series that correspond to our number by calculating $(i \gg 1)$. If $i$ is odd, we will have to add 4, since we are interested in the odd indices.
%This works because $(i \gg 8)$ calculates which word in the final , which is multiplied by 4.
\end{proof}
If we were to use a word size that is large enough such that lines 17-19 get run again, then we would have to find a more complex computation to find the correct index. This will only happen when $d\geq16384$, so we can safely assume that this will not be relevant in the near future unless new computers get invented with such large word sizes.
\subsection{Time Complexity Analysis}
We can now move on to showing the time complexity of the algorithm. 
First, we can try to find the running time of the first for-loop. At each iteration $i$, we will describe the amount of elements in $S^{(i)}$ as $n^{(i)}$ with $n=n^{(0)}$. We can then describe $n^{(i)}$ like so:
\begin{equation}
    n^{(i)}=\lceil \frac{n}{2^{i-l(i)}} \rceil
\end{equation}
which means that the first for-loop in total takes
$$\sum_{i=2}^{\log_2(d)}\lceil \frac{n}{2^{i-l(i)}} \rceil$$
Since the last loop iterates over every element of the original set, it must take $O(n)$ time. We can now derive the total running time:
$$O(n) + \sum_{i=2}^{\log_2(d)}\lceil \frac{n}{2^{i-l(i)}} \rceil$$
$$=O(n) + \sum_{i=2}^{\log_2(d)}\lceil n2^{l(i)-i} \rceil$$
$$\leq O(n) + \sum_{i=2}^{\log_2(d)}(n2^{l(i)-i} + 1)$$
$$\leq O(n + \log(d)) + n\sum_{i=2}^{\log_2(d)}2^{l(i)-i}$$
Now we use the fact that $l(i)$ is always the smallest number that satisfies $2^{l(i)} \geq i + 2$. Since it is the smallest number, then $2^{l(i)} < 2(i+2)$, which can be proved by contradiction: $2^{l(i)} \geq 2(i+2) \implies 2^{l(i)-1} \geq i+2$, which shows that there is a number that is 1 smaller than $l(i)$ that upholds the bound $2^{l(i)} \geq i + 2$. Therefore $2^{l(i)} < 2(i+2)$. We can now use this fact:
$$\leq O(n + \log(d)) + n\sum_{i=0}^{\infty}\frac{2(i+2)}{2^{i}} = O(n+\log(d))$$
The running time of the algorithm can therefore be bound to $O(n + \log(d))$.
