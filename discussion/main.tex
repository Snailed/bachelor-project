% Er mine resultater signifikante?
% Er der overensstemmelse af den teoretiske analyse og de praktiske resultater?
% Kan vi be- eller afkræfte vores hypoteser?
% Er der nogle mulige fejlkilder som kan have påvirket resultatet?
% Hvad er der af usikkerheder i benchmarkingen?
% Hvad kan vi fortælle omkring virkeligheden? Hvem vil have nytte af disse resultater?
\subsection{Discussion}
% Det er svært at komme med nogle super gode konklusioner omkring ordstørrelsens påvirkning på køretiden da jeg kun kunne køre med to forskellige ordstørrelser på min PC
Even though the results indicate a clear advantage to simpler algorithms to perform bit counting, the parallel bit counting algorithm should not be completely dismissed. While we can be pretty sure that a smaller $d < w$ might not improve the performance of the parallel bit-counting algorithm compared to the other algorithms, we do not know how the parallel algorithm performs on very large word-sizes (e.g. 256 to 384 bit words) as seen on some GPU units\cite{techpowerup}. Since the applications of the similarity search problem usually apply to very large data sets, it is often appropriate to perform highly efficient parallel calculations when possible. 
Parallel programming comes with many new challenges that were not considered in this project such as minimizing read/write operations to the main memory of the computer, the memory capacity of the graphics card and the size of the register files. 
This makes an actual implementation of the algorithm \ref{alg:parallel-d-and-c} non-trivial, especially since most programming interfaces for interacting with a GPU relies on intricate low-level systems programming (unless one were to use a language like Futhark\cite{futhark}). A nice feature of the algorithm is that it does not use multiplication, which means that many of the computations might be efficiently computed on a GPU.\\
Another thing that this experiment does not take into account is the index calculation time required to actually look up into the list of cardinalities. This was not included, since the correctness criteria of the algorithm may or may not take into account whether or not the cardinalities should have the same index in the final list as the input data.\\
The implementation that this experiment relied upon was optimized as per the best of my abilities, but futher optimizations might be possible. A nice feature of the Rust programming language is that the time complexity is very often documented in the documentation, which was very useful when working with heap-allocated vectors of numbers. The implementation was also written to be as generic as possible in regards to word size such that the experiment would be performed as unbiased as possible. This was possible due to Rust's generics, and while Rust design philosphy relies upon zero-cost abstractions\cite{rust-lang} (like C++), this might cause some optimization issues. Further optimizations might also be possible by manually analyzing the produced assembly code, which was regarded as out-of-scope for this project.\\
In general, while parallel bit counting theoretically is more efficient than its sequential and instruction-level counterparts, it has yet to be shown to be the case in practice. While further research might research how it performs on specialized hardware, the algorithm behind this will probably first be useful when larger word-sizes become more easily available and computable, if ever.
