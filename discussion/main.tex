% Er mine resultater signifikante?
% Er der overensstemmelse af den teoretiske analyse og de praktiske resultater?
% Kan vi be- eller afkræfte vores hypoteser?
% Er der nogle mulige fejlkilder som kan have påvirket resultatet?
% Hvad er der af usikkerheder i benchmarkingen?
% Hvad kan vi fortælle omkring virkeligheden? Hvem vil have nytte af disse resultater?
\section{Discussion}
% Det er svært at komme med nogle super gode konklusioner omkring ordstørrelsens påvirkning på køretiden da jeg kun kunne køre med to forskellige ordstørrelser på min PC
Even though the results indicate a clear advantage to simpler algorithms to perform bit counting, the parallel bit counting algorithm should not be completely dismissed. The experiment does not tell anything about how the algorithm performs on very large word size (e.g. 256 to 384 bit words) as seen on some GPU units\cite{techpowerup}. Since the applications of the similarity search problem usually apply to very large data sets, it is often appropriate to perform highly efficient parallel calculations when possible. Parallel programming comes with many new challenges that were not considered in this project such as minimizing read/write operations to the main memory of the computer, the memory capacity of the graphics card and the size of the register files. This makes an actual implementation of the algorithm \ref{alg:parallel-d-and-c} non-trivial, especially since most programming interfaces for interacting with a GPU relies on intricate low-level systems programming. A language like Futhark might ease this however\cite{futhark}.\\
Another thing that this experiment does not take into account is the index calculation time required to actually look up into the list of cardinalities. This was not included, since the correctness criteria of the algorithm may or may not take into account whether or not the cardinalities should have the same index in the final list as the input data. A general formula for this was not found, so the calculation time cannot be shown to be asymptotically positive in regards to the word size (although one might expect it to be), which makes any results unconclusive to real life applications anyways.\\
The implementation that this experiment relied upon was optimized as per the best of my abilities, but futher optimizations might be possible. A nice feature of the Rust programming language is that the asymptotic run time very often is documented in the documentation, which was very useful when working with heap-allocated vectors of numbers. The implementation was also written to be as generic as possible in regards to word size such that the experiment would be performed as unbiased as possible. This was possible due to Rust's generics, and while Rust design philosphy relies upon zero-cost abstractions\cite{rust-lang} (like C++), this might cause some optimization issues. Further optimizations might also be possible by manually analyzing the produced assembly code, which was regarded as out-of-scope for this project.\\
In general, while parallel bit counting theoretically is more efficient than its sequential and instruction-level counterparts, it has yet to be shown to be the case in practice. While further research might research how it performs on specialized hardware, the algorithm behind this will probably first be useful when larger word-sizes become more easily available and computable, if ever.
