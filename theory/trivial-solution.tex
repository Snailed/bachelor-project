\subsection{Trivial Solution}
The most obvious solution is to compare each element in each set $A\in \mathcal{F}$ with each element in $Q$ to calculate the size of the intersection $|A\cap U|$ and union $|A \cup U|$. This solution will always give the correct answer, and query in $O(n|Q|\max_{A\in \mathcal{F}}|A|)$ time trivially, but suffers from the \textit{curse of dimensionality}. As the amount of dimensions in the data set doubles, the running time quadruples! When working with high-dimensional data sets like often seen in text processing, this can have huge consequences. \citet{li2011hashing} has shown that it is easy to reach a dimensionality upwards of $|A|=2^{83}$ when using \textit{5-shingles} (5 contiguous words) of the 10,000 most common English words.
A simple improvement is to make a hash table for $Q$ of size $\max_{A\in \mathcal{F}}|A|$ with no collisions and look up every entry in $A$ to find any matches. This could reduce the running time to $O(|Q|+n\max_{A\in \mathcal{F}}|A|)$ per query.\\
These algorithms are both guaranteed to return the set $S=\arg\max_{A \in \mathcal{F}}J(Q,A)$ on every query. The next algorithm can query much faster than this by relaxing this condition: If we allow pre-processing the data and relax the requirement of finding an exact match by considering the approximate similarity search problem instead, we can create so-called sketches of the data set before querying. This sketching strategy can drastically reduce the query time.
