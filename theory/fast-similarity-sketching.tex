% Hvad er pointen med Fast Similarity Sketching
% Hvad har Dahlgaard introduceret? Hvordan gør de det sublineært? Hvad er de overordnede linjer for beviset?
% Hvad har Knudsen introduceret? Hvad er den teoretiske forskel?
\subsection{Later Improvements}
Many improvements have since been discovered.
\citet{dahlgaard2017fast} have introduced an improved sketch, which is much faster to create and preserves the same properties as the MinHash sketch, while reducing the query time to $O(k\cdot l + |Q|)=O(n^\rho \log_2(n) + |Q|)$. Part of this improvement is due to a special algorithm that can eliminate "bad matches" by estimating if the Jaccard similarity between the query set and candidate set is above some threshold within a certain probability. Here, a "bad match" is defined as a candidate set $A$ where $J(A, Q) < j_2$. 
This reduces the filtering step to only take $O(l + |Q|)$ time, since the bad matches can be eliminated in $O(l)$ time and the amount of matches remaining is $O(1)$. \citet{christiani2018fast} has since improved this query time to $O(n^\rho + |Q|)$. \\
Until now, the data structures presented error with a constant probability $O(1)$. If one wants to reach a better error probability $\varepsilon$ (could for example be $\varepsilon = O(1/n)$), it is common practice within the field to use $O(\log_2(1/\varepsilon))$ independent data structures. This will usually result in a query time of $O((n^\rho + |Q|)\log_2(1/\varepsilon))$. \\
\citet{fast-similarity-search} has shown a method of achieving an improved $O(n^\rho \log_2(1/\varepsilon)+|Q|)$ query time instead, which is very beneficial when working with highly dimensional datasets and a small $\varepsilon$. This is done by creating the $M$ different sketches of $Q$ nescessary for querying from one big sketch of size $M\cdot \log^3_2(n)$, with each sketch segment fed into each LSH datastructure.\\
\citet{fast-similarity-search} has further improved the query time to $O((\frac{n\log_2(d)}{d})^{\rho}\log_2(1/\varepsilon)+|Q|)$ (where $d$ is the word-size) by reducing $k$ such that $h_i\in \mathcal{H}$ becomes $(j_1, j_2, O((n/b)^\rho), n/b)$-sensitive where $b=d/\log_2(d)$ and then efficiently filtering through the $b$ expected "bad" matches. \\
Understanding why this filtering is efficient requires understanding the $b$-bit minwise hashing trick presented by \citet{li2011hashing}:
By only storing the $b$ lowest bits of each hash value, one can pack $O(d/b)$ hash values per word. \citet{fast-similarity-search} uses 1-bit minwise hashing to create a sketch of size $O(\log_2(d))$ per set $A\in \mathcal{F}$.
While 1-bit hashing greatly increases the chance of collision per hash value, it allows us to use many more hash values per sketch, which is suprisingly effective at negating this \cite{li2011hashing}.
This allows us to store $O(\frac{d}{\log_2(d)})$ sketches per word. When querying, instead of comparing our sketches of Q with each of the $O(n^\rho)$ sketches per data structure one at a time, we can compare it to $O(\frac{d}{\log_2(d)})$ different sketches per operation. \\
By calculating the amount of bits per sketch for $A$ and $Q$ that are equal, one can estimate the Jaccard Similarity to a degree which is good enough to filter a candidate match.\\
This calculation can be done by bitwise XNOR'ing a word packed with sketches of a set $A\in \mathcal{F}$ with a word packed with sketches of $Q$. Then, one can calculate the amount of bits set. Every bit set will then indicate that $Q$ and $A$ hashed to the same value, which means that we can filter each match based on whether or not the total amount of bits set is over some threshold.\\
It is this calculation on how to count these bits efficiently that the rest of this project will focus on.
