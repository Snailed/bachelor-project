% Hvad er pointen med Fast Similarity Sketching
% Hvad har Dahlgaard introduceret? Hvordan gør de det sublineært? Hvad er de overordnede linjer for beviset?
% Hvad har Knudsen introduceret? Hvad er den teoretiske forskel?
\subsection{Later Improvements}
Many improvements have since been discovered.
\citet{andoni2006efficient} have since improved the total query time to $O(n^\rho|Q|)$ by using a technique called \textit{tensoring}. Tensoring entails creating a collection of $m=o(l)$ sketch functions instead of the usual $k$ by picking $m$ functions from $\mathcal{H}$ uniformly at random. Let each $g_i$for $i\in [l]$ from the LSH framework now be a distinct combination of these $m$ hash functions of size $t$ (where $t$ is a positive integer). It is now possible to choose $t$ such that ${m \choose t} = l$, such that every combination of the $m$ hash values are used. By doing this, it is only nescessary to compute $O(l)$ hash values of $Q$ which is a reduction by a factor of $k$ \cite{andoni2006efficient}\cite{christiani2018fast}.\\
\citet{dahlgaard2017fast} have also introduced an improved sketch called the \textit{Fast Similarity Sketch} (FSS), which is much faster to create and preserves the same properties as the MinHash sketch, while reducing the query time to $O(k\cdot l + |Q|)=O(n^\rho \log_2(n) + |Q|)$. \\
This improvement in query time is due to multiple factors.\\
First of all, a sketch of size $k$ of a set $A$ takes an expected running time of $O(k\log_2(k) + |A|)$ to create using the FSS algorithm, which is improved compared to the $O(k|A|)$ running time of the classic MinHash algorithm.\\
Secondly, the algorithm uses a single intermediate sketch that it samples the hash values of the sketch from, where the size of the intermediate sketch is independent of the size of the query set, which turns the time to create the sketch of $Q$ from $O(n^\rho \log_2(n)|Q|)$ to $O(n^\rho \log_2(n) + |Q|)$.\\
Thirdly, the algorithm also uses a faster method for eliminating ``bad matches'' from the $O(l)$ candidate sets by estimating if the Jaccard similarity between the query set and candidate set is above some threshold within a certain probability. Here, a "bad match" is defined as a candidate set $A$ where $J(A, Q) < j_2$.
This reduces the filtering step to only take $O(l + |Q|)$ time, since the bad matches can be eliminated in $O(l)$ time and the amount of matches remaining is $O(1)$. These $O(1)$ matches can then finally be compared with to the query set which takes $O(|Q|)$ time.\\
These three factors result in a $O(n^\rho \log_2(n) + |Q|)$ total query time, which can be combined with tensoring to a $O(n^\rho + |Q|)$ query time as shown by \citet{christiani2018fast}.\\
Until now, the data structures presented error with a constant probability $O(1)$. If one wants to reach a better error probability $\varepsilon$ (could for example be $\varepsilon = O(1/n)$), it is common practice within the field to use $M=\Theta(\log_2(1/\varepsilon))$ independent data structures. This will usually result in a query time of $O((n^\rho + |Q|)\log_2(1/\varepsilon))$. \\
\citet{fast-similarity-search} has shown a method of achieving an improved $O(n^\rho \log_2(1/\varepsilon)+|Q|)$ query time instead, which is very beneficial when working with highly dimensional datasets and a small $\varepsilon$. This is done by sampling the $\Theta(M\cdot l)$ different sketches of $Q$ nescessary for querying from a single sketch of size $\Theta(M\cdot \log^2_2(n))$, with each sketch segment fed into a LSH datastructure. \\
\citet{fast-similarity-search} has further improved the query time to $O((\frac{n\log_2(d)}{d})^{\rho}\log_2(1/\varepsilon)+|Q|)$ (where $d$ is the word-size) by reducing $k$ such that $h_i\in \mathcal{H}$ becomes $(j_1, j_2, O((n/b)^\rho), n/b)$-sensitive where $b=d/\log_2(d)$. Then, using a filtering algorithm inspired by \cite{fast-similarity-search}, each of the $l$ data-structures will pick either a candidate set $A\in \mathcal{F}$ which is estimated to have $J(A,Q) \geq j_2$ or return none. \\
From the $\leq M$ different candidate sets provided by the $M$ data structures, one needs to filter out those data structures that have a high probability of returning a ``bad'' set.
If the amount of candidate sets are above a specific threshold, \citet{fast-similarity-search} shows that one can pick a random candidate set which will be correct with the required probability. If the amount of candidate sets is smaller than the threshold however, one needs to filter out the bad sets efficiently. 
This is done by keeping another sketch $P(A)$ for each $A\in \mathcal{F}$ of $\Theta(\max(\log_2(n), \log_2(1/\varepsilon)))$ hash values during the pre-processing step. For each of these hash values, only the least significant bit will be stored (also known as the $b$-bit minwise hashing trick\cite{li2011hashing}) as well as a set of $d=O(\log_2(w))$ coordinates of $P$. By comparing the sketch $P(A)$ with $P(Q)$ for each of the $l$ candidate sets, we can eliminate bad matches with a probability that is high enough to achieve the desired error bounds. Since each sketch requires $d$ bits, we can embed $O(w/\log_2(w))$ sketches into each word.
When querying, instead of comparing our sketches of $Q$ with each of the $l$ candidate sketches one at a time, we can compare it to $O(\frac{d}{\log_2(d)})$ different sketches at once. \\
By calculating the amount of bits per sketch for $A$ and $Q$ that are equal, one can estimate the Jaccard Similarity to a degree which is good enough to filter a candidate match.\\
This calculation can be done by bitwise XNOR'ing a word packed with sketches of a set $A\in \mathcal{F}$ with a word packed with sketches of $Q$. Then, one can calculate the amount of bits set. Every bit set will then indicate that $Q$ and $A$ hashed to the same value, which means that we can filter each match based on whether or not the total amount of bits set is over some threshold.\\
It is this calculation on how to count these bits efficiently that the rest of this project will focus on.
