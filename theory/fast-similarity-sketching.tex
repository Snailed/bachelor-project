% Hvad er pointen med Fast Similarity Sketching
% Hvad har Dahlgaard introduceret? Hvordan gør de det sublineært? Hvad er de overordnede linjer for beviset?
% Hvad har Knudsen introduceret? Hvad er den teoretiske forskel?
\subsection{Later Improvements}
Many improvements have since been discovered. The last step of each query of the LSH framework results in an expected $O(L)$ false positives that needs to be filtered out. By using an intermediate sketch of size $O(\log_2^3(n))$, it is possible to reduce the query time to $O((l\cdot |Q|)\log_2^3(n))$ by sampling from the intermediate sketch instead of the regular sketch \cite{dahlgaard2017fast}.
\citet{dahlgaard2017fast} have introduced an improved sketch, which is much faster to create and preserves the same properties as the MinHash sketch, while reducing the query time to $O(k\cdot l + |Q|)=O(n^\rho \log_2(n) + |Q|)$. Part of this improvement is due to a special algorithm that can eliminate bad matches by estimating if the Jaccard similarity between the query set and candidate set is above some threshold without calculating the actual Jaccard similarity. This reduces the filtering step to only take $O(l + |Q|)$ time, since the bad matches can be eliminated in $O(l)$ time and the amount of matches remaining is $O(1)$. \citet{christiani2018fast} has since improved this query time to $O(n^\rho + |Q|)$. \\
Until now, the data structures presented error with a constant probability $O(1)$. If one wants to reach a better error probability $1>\varepsilon>0$, it is common practice within the field to use $O(\log_2(1/\varepsilon))$ independent data structures. This will usually result in a query time of $O((n^\rho + |Q|)\log_2(1/\varepsilon))$. \\
\citet{fast-similarity-search} has shown a method of achieving an improved $O(n^\rho \log_2(1/\varepsilon)+|Q|)$ query time instead, which is very beneficial when working with highly dimensional datasets and a small $\varepsilon$. This is done by creating the $M$ different sketches of $Q$ nescessary for querying from one big sketch of size $M\cdot \log^3_2(n)$, with each sketch segment fed into each LSH datastructure.\\
\citet{fast-similarity-search} has further improved the query time to $O((\frac{n\log_2(w)}{w})^{\rho}\log_2(1/\varepsilon)+|Q|)$ by reducing $k$ such that $h_i\in \mathcal{H}$ becomes $(j_1, j_2, O((n/b)^\rho), n/b)$-sensitive where $b=w/\log_2w$ and then efficiently filtering through the $b$ expected "bad" matches. \\
Understanding why this filtering is efficient requires understanding the $b$-bit minwise hashing trick presented by \citet{li2011hashing}:
By only storing the $b$ lowest bits of each hash value, one can pack $O(\log_2(w)/b)$ hash values per word (where $w$ is the word size). \citet{fast-similarity-search} uses 1-bit minwise hashing to create a sketch of size $O(\log_2(w))$ per set $A\in \mathcal{F}$. This allows us to store $O(\frac{w}{\log_2(w)})$ sketches per word. \\
By calculating the amount of bits per sketch for $A$ and $Q$ that are equal, one can estimate the Jaccard Similarity to a degree which is good enough to filter a candidate match.\\
This calculation can be done by bitwise XOR'ing a word packed with sketches of different sets $A\in \mathcal{F}$ with a word packed with sketches of $Q$. Then, one can calculate the amount of bits set per word. Every bit set will then indicate that $Q$ and $A$ hashed to the same value, which means that we can filter each match based on whether or not the total amount of bits set is over some threshold.\\
It is this calculation on how to count these bits efficiently that the rest of this project will focus on.
